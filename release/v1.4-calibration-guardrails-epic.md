# Epic: Calibration Confidence Guardrails

Date: 2026-02-17  
Epic Branch: `test/v1.4-calibration-hardening`  
Parent Release: `release/v1.4`

## Problem

Current calibration-aware risk controls can be over-trusted when data is sparse or over-segmented.

Primary risks:
1. Sample-size illusion (too few resolved predictions).
2. Over-conditioning into tiny buckets.
3. Premature coupling of calibration signals to risk scaling.

## Goal

Make calibration-aware sizing statistically safer without removing LLM-driven hypothesis generation.

## Product Principles

1. LLM drives idea generation and reasoning, not direct risk leverage.
2. Deterministic risk controls require evidence thresholds.
3. Positive calibration scaling requires stronger evidence than negative scaling.

## Scope

1. Minimum sample-size gates for calibration effects.
2. Hierarchical bucket blending (shrinkage to parent stats).
3. Asymmetric multiplier policy (downweight aggressive, upweight conservative).
4. Uncertainty-aware gating for positive multipliers.
5. Slow policy cadence + hysteresis (anti-thrash).
6. Acceptance and regression tests across these controls.

## Out of Scope

1. Replacing the LLM strategy layer.
2. New exchanges or execution adapters.
3. New user-facing channels.

## Success Criteria

1. No calibration-based risk increase below defined evidence thresholds.
2. Small buckets inherit parent priors instead of driving risk alone.
3. Positive scaling only occurs when uncertainty bounds support real edge.
4. Multiplier changes are stable over time (no per-trade oscillation).
5. Full test suite passes with new calibration guardrail tests.

