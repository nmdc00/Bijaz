# Release v1.6.1 - LLM Burst Control and Interactive Reliability

Date: 2026-02-17  
Status: In Progress

## Problem

Interactive Telegram turns were stalling at planning while local trivial-task calls timed out and immediately cascaded to remote fallback.  
When heartbeat/proactive jobs overlapped with active chat windows, this amplified provider burst traffic and triggered OpenAI cooldown/rate-limit loops.

## Objectives

1. Reduce non-critical LLM burst amplification.
2. Keep interactive responses responsive under background scheduler load.
3. Preserve deterministic/mechanical fallback behavior when non-critical LLM calls are suppressed.

## Implemented Changes

1. Non-critical fallback suppression by reason:
- Added `agent.nonCriticalFallbackSuppressReasons`.
- Non-critical fallback is now suppressible for configured reasons (`info_digest`, `session_compaction`, `proactive_query_refine`, `proactive_follow_up_queries`).

2. Non-critical reason cooldown throttles:
- Added `agent.nonCriticalReasonCooldownSeconds` with reason-specific cooldown windows.
- Repeated non-critical calls inside cooldown are skipped early with explicit logs.

3. Active-chat suppression windows for scheduled jobs:
- Added `notifications.proactiveSearch.suppressLlmDuringActiveChatSeconds`.
- Added `notifications.heartbeat.suppressLlmDuringActiveChatSeconds`.
- During active chat windows, proactive LLM refinement is disabled and heartbeat LLM generation is deferred.

## Expected Outcome

1. Fewer local-timeout -> remote-fallback cascades.
2. Lower chance of OpenAI rate-limit bursts during active chat.
3. Improved p95 interactive response reliability without removing autonomy loops.
